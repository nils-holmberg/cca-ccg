{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "e19a0856-d3b4-54b6-bdc2-01053b99a2c2",
        "openai_ephemeral_user_id": "d3846df0-f8c4-59ed-9a63-7178ba7b3db3",
        "openai_subdivision1_iso_code": "SE-M"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "noteable": {
      "last_transaction_id": "6b00e5d8-9a94-45dd-962e-3d317e771d9f"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "45d09c3f-8e35-4092-976b-18e0388831a1",
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "execution_count": null,
      "source": "",
      "outputs": []
    },
    {
      "id": "9afbae7c-e78c-4d78-ab2a-94fa9a4c42a1",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "502ee840-bd50-45c6-a74a-d83f27738875"
        },
        "ExecuteTime": {
          "end_time": "2023-08-18T12:01:03.960044+00:00",
          "start_time": "2023-08-18T11:59:06.137229+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install git+https://github.com/openai/whisper.git",
      "outputs": []
    },
    {
      "id": "23118b88-e0e8-4a11-9147-124b71953fd8",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "cb0f9c21-8b58-4e20-98bb-ba16a7c82cf5"
        },
        "ExecuteTime": {
          "end_time": "2023-08-18T12:04:49.910754+00:00",
          "start_time": "2023-08-18T12:04:46.824370+00:00"
        }
      },
      "execution_count": null,
      "source": "import torch\nfrom whisper import WhisperASR\n\n# Load the Whisper ASR model\nmodel = WhisperASR()\n\n# Load the audio file\naudio_path = 'res/en-clip.wav'\naudio = torch.from_numpy(model.load_audio(audio_path)).unsqueeze(0)\n\n# Transcribe the audio\ntranscription = model.transcribe(audio)\ntranscription",
      "outputs": []
    },
    {
      "id": "f8ed2d5e-73f7-4338-9050-3aa50fb49295",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ba70d790-5909-4560-b9b8-714c35cde954"
        },
        "ExecuteTime": {
          "end_time": "2023-08-18T12:05:09.335873+00:00",
          "start_time": "2023-08-18T12:05:09.175011+00:00"
        }
      },
      "execution_count": null,
      "source": "import torch\nimport whisper\n\n# Checking the available attributes in the whisper package to identify the correct module for ASR\ndir(whisper)",
      "outputs": []
    },
    {
      "id": "fd839b8f-cd32-4311-8d12-acfc14b62e12",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "49afbcfa-9f7a-4287-bf88-b6481919038e"
        },
        "ExecuteTime": {
          "end_time": "2023-08-18T12:05:29.243199+00:00",
          "start_time": "2023-08-18T12:05:29.062848+00:00"
        }
      },
      "execution_count": null,
      "source": "from whisper import Whisper\n\n# Load the Whisper ASR model\nmodel = Whisper()\n\n# Transcribe the audio\ntranscription = model.transcribe(audio_path)\ntranscription",
      "outputs": []
    },
    {
      "id": "67d3f37b-abc4-4ba7-b485-98fa6c24ca03",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "df9b5340-3745-4fa2-91b5-51315c531ca4"
        },
        "ExecuteTime": {
          "end_time": "2023-08-18T12:05:50.019529+00:00",
          "start_time": "2023-08-18T12:05:49.841592+00:00"
        }
      },
      "execution_count": null,
      "source": "# Checking the documentation for the Whisper class to understand its initialization and usage\nhelp(Whisper)",
      "outputs": []
    },
    {
      "id": "4b2fc5c3-57d6-4956-bfae-eed41c53c96c",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "4180d021-8e5a-4b9d-9570-ce2e42e54447"
        },
        "ExecuteTime": {
          "end_time": "2023-08-18T12:06:17.062099+00:00",
          "start_time": "2023-08-18T12:06:16.902282+00:00"
        }
      },
      "execution_count": null,
      "source": "# Checking available pre-trained models in the whisper package\nwhisper.available_models()",
      "outputs": []
    },
    {
      "id": "88444aa8-2a5f-4142-90f1-5ec18608ba7b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "49e21afb-3c3e-473d-84ae-5be43007cd94"
        },
        "ExecuteTime": {
          "end_time": "2023-08-18T12:06:43.033422+00:00",
          "start_time": "2023-08-18T12:06:42.803605+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the 'base.en' model from the available models\nmodel = whisper.Whisper('base.en')\n\n# Transcribe the audio\ntranscription = model.transcribe(audio_path)\ntranscription",
      "outputs": []
    },
    {
      "id": "d0f41937-4f69-4810-999d-409a2baa5a2b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "6f701249-23d9-4fc1-886a-1127983089b7"
        },
        "ExecuteTime": {
          "end_time": "2023-08-18T12:07:10.072328+00:00",
          "start_time": "2023-08-18T12:07:09.891256+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the 'base.en' model dimensions and then initialize the Whisper model with those dimensions\ndims = whisper.ModelDimensions('base.en')\nmodel = whisper.Whisper(dims)\n\n# Transcribe the audio\ntranscription = model.transcribe(audio_path)\ntranscription",
      "outputs": []
    },
    {
      "id": "37da3764-3ba4-42be-a32e-07eb87164eae",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "f02611df-cebd-4b19-9026-701b433b854f"
        },
        "ExecuteTime": {
          "end_time": "2023-08-18T12:09:27.311952+00:00",
          "start_time": "2023-08-18T12:09:15.461816+00:00"
        }
      },
      "execution_count": null,
      "source": "import whisper\n\n# Load the 'base' model using the provided code\nmodel = whisper.load_model(\"base\")\n\n# Transcribe the audio\ntranscription = model.transcribe(audio_path)\ntranscription",
      "outputs": []
    },
    {
      "id": "9139f33f-e680-4887-8d29-10d359f25acf",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1cb51d23-929b-4827-a814-1000c95fda78"
        },
        "ExecuteTime": {
          "end_time": "2023-08-18T12:11:55.872643+00:00",
          "start_time": "2023-08-18T12:11:54.920430+00:00"
        }
      },
      "execution_count": null,
      "source": "# Transcribe the English audio file in the 'res' folder using the loaded 'base' model\naudio_path = 'res/en-clip.wav'\ntranscription = model.transcribe(audio_path)\ntranscription",
      "outputs": []
    }
  ]
}